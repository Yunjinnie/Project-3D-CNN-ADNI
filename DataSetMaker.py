# -*- coding: utf-8 -*-
"""dataset.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13tstR4llYVXJeBqCA1snBmwW_OrDGr-j
"""

## ======= load modules ======= ##
import glob
import os

import numpy as np
import pandas as pd

from monai.data import ImageDataset
from monai.transforms import AddChannel, Compose, RandRotate90, Resize, ScaleIntensity, Flip, ToTensor


## To use DataSetMaker, please make an instance with args & call a method named 'make_dataset()'
## YOU SHOULD CHECK "data_dir", "subject_dir", "control_csv_name", "case_csv_name", "target"
## A function that changes control/case ratio will be implemented later

##### code example #####
## from DataSetMaker import DataSetMaker
## datasetMaker = DataSetMaker(args)
## partition = datasetMaker.make_dataset()


class DataSetMaker():
    def __init__(self,args):
        self.data_dir =         '/home/connectome/dhkdgmlghks/3DCNN/data/2.sMRI_freesurfer'
        self.subject_dir =      '/home/connectome/dhkdgmlghks/3DCNN/data/3.demo_qc'
        self.control_csv_name = 'ABCD_suicide_control.csv'
        self.case_csv_name =    'ABCD_suicide_case.csv'
        self.target =           'SuicideAll'
        self.args = args
        
        
    def make_dataset(self):
        self.load_images()
        self.load_control_case_list()
        self.partition()
        
        return self.dataset
    
    
    ### load image files (subject ID + '.npy') as list
    def load_images(self):
        os.chdir(self.data_dir)
        self.images_npy = glob.glob('*.npy')
        self.images_npy = sorted(self.images_npy)
        self.images_npy = pd.Series(self.images_npy)
        self.images_subjectkey = self.images_npy.map(lambda x: x[:-4])
        self.image_files = pd.DataFrame({'filename':self.images_npy, 'subjectkey':self.images_subjectkey})
        print("*** Loading image files as list is completed ***")

        
    ### load control & case subjects & preprocessing
    def load_control_case_list(self):
        os.chdir(self.subject_dir)
        self.control_data = pd.read_csv(self.control_csv_name)
        self.control_data = self.control_data.loc[:,['subjectkey',self.target]]
        self.control_data = self.control_data.sort_values(by='subjectkey')
        self.control_data = self.control_data.dropna(axis = 0)
        self.control_data = self.control_data.reset_index(drop=True) # removing subject have NA values in sex

        self.case_data = pd.read_csv(self.case_csv_name)
        self.case_data = self.case_data.loc[:,['subjectkey',self.target]]
        self.case_data = self.case_data.sort_values(by='subjectkey')
        self.case_data = self.case_data.dropna(axis = 0)
        self.case_data = self.case_data.reset_index(drop=True) # removing subject have NA values in sex
        print("*** Loading control & case subject list is completed ***")
        

    ### make train/test/validation dataset
    def partition(self):
        os.chdir(self.data_dir) # if I do not change directory here, image data is not loaded
        # get subject ID and target variables as sorted list
        
        self.case_merged =    pd.merge(self.image_files, self.case_data, how='inner',on='subjectkey')
        self.control_merged = pd.merge(self.image_files, self.control_data, how='inner',on='subjectkey')
                      
        self.n_case_total = len(self.case_merged)
        self.n_case_val =   int(self.n_case_total * self.args.val_size)
        self.n_case_test =  int(self.n_case_total * self.args.test_size)
        self.n_case_train = self.n_case_total - (self.n_case_val+self.n_case_test)
        
        self.n_control_total = len(self.control_merged)
        self.n_control_val =   self.n_case_val
        self.n_control_test =  self.n_case_test
        self.n_control_train = self.n_control_total - (self.n_control_val+self.n_control_test)
 
        control_val, control_test, control_train = np.split(self.control_merged, [self.n_control_val, self.n_control_val+self.n_control_test])
        case_val,    case_test,    case_train =    np.split(self.case_merged,    [self.n_case_val, self.n_case_val+self.n_case_test])
        
        total_train = control_train.append(case_train,ignore_index=True)
        total_val =   control_val.append(case_val,ignore_index=True)
        total_test =  control_test.append(case_test,ignore_index=True)
                                                
        resize = self.args.resize
        train_transform = Compose([ScaleIntensity(),
                                   AddChannel(),
                                   Resize(resize),
                                  ToTensor()])

        val_transform = Compose([ScaleIntensity(),
                                   AddChannel(),
                                   Resize(resize),
                                  ToTensor()])

        test_transform = Compose([ScaleIntensity(),
                                   AddChannel(),
                                   Resize(resize),
                                  ToTensor()])                                                
        
        train_set = ImageDataset(image_files=total_train['filename'], labels=total_train[self.target], transform=train_transform)
        val_set =   ImageDataset(image_files=total_val['filename'],   labels=total_train[self.target], transform=val_transform)
        test_set =  ImageDataset(image_files=total_test['filename'],  labels=total_train[self.target], transform=test_transform)

        self.dataset = {}
        self.dataset['train'] = train_set
        self.dataset['val'] = val_set
        self.dataset['test'] = test_set
        print("*** Splitting data into train, val, test is completed ***")