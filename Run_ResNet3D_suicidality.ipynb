{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Run_ResNet3D_suicidality.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPocXfvuDfiPIqxLXCcvGFR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"rO1Xj_cyg36d"},"outputs":[],"source":["## =================================== ##\n","## ======= ResNet ======= ##\n","## =================================== ##\n","\n","'''\n","__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n","           'resnet152', 'resnext50_32x4d', 'resnext101_32x8d',\n","           'wide_resnet50_2', 'wide_resnet101_2']\n","'''"]},{"cell_type":"code","source":["## ======= load module ======= ##\n","import glob\n","import os\n","from os import listdir\n","from os.path import isfile, join\n","from tqdm.auto import tqdm ##progress\n","import time\n","import math\n","import random\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn.functional as F\n","from torchvision import transforms, utils\n","import torch.optim as optim\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import matplotlib\n","import seaborn as sns\n","\n","import random\n","import hashlib\n","import json\n","\n","from sklearn.metrics import confusion_matrix, roc_auc_score\n","\n","import monai ##monai: medical open network for AI\n","from monai.data import CSVSaver, ImageDataset, DistributedWeightedRandomSampler\n","from monai.transforms import AddChannel, Compose, RandRotate90, Resize, ScaleIntensity, Flip, ToTensor\n","from monai.utils import set_determinism\n","from monai.apps import CrossValidation\n","\n","import argparse\n","from copy import deepcopy ## add deepcopy for args\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","import resnet3d\n","from DataSetMaker import DataSetMaker\n","## =================================== ##"],"metadata":{"id":"xeCX-oynhLW6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## ========= Argument Setting ========= ##\n","parser = argparse.ArgumentParser()\n","\n","#parser.add_argument(\"--GPU_NUM\",default=1,type=int,required=True,help='')\n","parser.add_argument(\"--model\",required=True,type=str,choices=['resnet50', 'resnet101', 'resnet152'],help='')\n","parser.add_argument(\"--val_size\",default=0.1,type=float,required=False,help='')\n","parser.add_argument(\"--test_size\",default=0.1,type=float,required=False,help='')\n","parser.add_argument(\"--resize\",default=(96,96,96),required=False,help='')\n","parser.add_argument(\"--train_batch_size\",default=32,type=int,required=False,help='')\n","parser.add_argument(\"--val_batch_size\",default=8,type=int,required=False,help='')\n","parser.add_argument(\"--test_batch_size\",default=1,type=int,required=False,help='')\n","parser.add_argument(\"--optim\",type=str,required=True,help='', choices=['Adam','SGD'])\n","parser.add_argument(\"--lr\", default=0.01,type=float,required=False,help='')\n","parser.add_argument(\"--momentum\", default=0.5,type=float,required=False,help='')\n","parser.add_argument(\"--weight_decay\",default=0.001,type=float,required=False,help='')\n","parser.add_argument(\"--epoch\",type=int,required=True,help='')\n","parser.add_argument(\"--exp_name\",type=str,required=True,help='')\n","parser.add_argument(\"--gpu_ids\",type=int,nargs='*',required=True,help='')\n","## ==================================== ##"],"metadata":{"id":"jlYeYzyfhNAJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## ========= GPU Setting ========= ##\n","#GPU_NUM = args.GPU_NUM # enter the number you want to use GPU\n","#GPU_NUM=1 # ***\n","#device = 'cpu'\n","#device = 'cuda:1'\n","#device = torch.device(f'cuda:{GPU_NUM}' if torch.cuda.is_available() else 'cpu')\n","\n","#torch.cuda.set_device(device)\n","#print('Experiment is performed on GPU {}'.format(torch.cuda.current_device()))\n","## ==================================== ##"],"metadata":{"id":"BGA6vsRLhOnM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["command = \"--model densenet121 --optim Adam --lr 1e-6 --epoch 20 --exp_name dense01 --gpu_ids 0 1 --train_batch_size 16 --val_batch_size 64 --test_batch_size 1\"\n","command = command.split(\" \")\n","args = parser.parse_args(args=command)\n","\n","datasetMaker = DataSetMaker(args)\n","partition = datasetMaker.make_dataset()"],"metadata":{"id":"DPmf6gZkit-Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## ========= Train,Validate, and Test ========= ##\n","# define training step\n","def train(net,partition,optimizer,criterion,args):\n","    trainloader = torch.utils.data.DataLoader(partition['train'],\n","                                             batch_size=args.train_batch_size,\n","                                             shuffle=True,\n","                                             num_workers=2)\n","\n","    net.train()\n","\n","    correct = 0\n","    total = 0\n","    train_loss = 0.0\n","\n","\n","    for i, data in enumerate(trainloader,0):\n","        optimizer.zero_grad() #this code makes {train gradient=0}\n","        image, label = data\n","        image = image.to(f'cuda:{net.device_ids[0]}')\n","        label = label.to(f'cuda:{net.device_ids[0]}')\n","        output = net(image)\n","\n","        loss = criterion(output,label)\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","        _, predicted = torch.max(net(image).data,1)\n","        total += label.size(0)\n","        correct += (predicted == label).sum().item()\n","\n","    train_loss = train_loss / len(trainloader)\n","    train_acc = 100 * correct / total\n","\n","    return net, train_loss, train_acc\n","\n","\n","# define validation step\n","def validate(net,partition,criterion, scheduler,args):\n","    valloader = torch.utils.data.DataLoader(partition['val'],\n","                                           batch_size=args.val_batch_size,\n","                                           shuffle=False,\n","                                           num_workers=2)\n","\n","    net.eval()\n","\n","    correct = 0\n","    total = 0\n","    val_loss = 0.0\n","\n","    with torch.no_grad():\n","\n","        for i, data in enumerate(valloader,0):\n","            image, label = data\n","            image = image.to(f'cuda:{net.device_ids[0]}')\n","            label = label.to(f'cuda:{net.device_ids[0]}')\n","            output = net(image)\n","\n","            loss = criterion(output,label)\n","\n","            val_loss += loss.item()\n","            _, predicted = torch.max(output.data,1)\n","            total += label.size(0)\n","            correct += (predicted == label).sum().item()\n","\n","        val_loss = val_loss / len(valloader)\n","        val_acc = 100 * correct / total\n","\n","    scheduler.step(val_acc)\n","    return val_loss, val_acc\n","\n","\n","# define test step\n","def test(net,partition,args):\n","    testloader = torch.utils.data.DataLoader(partition['test'],\n","                                            batch_size=args.test_batch_size,\n","                                            shuffle=False,\n","                                            num_workers=2)\n","\n","    net.eval()\n","\n","    correct = 0\n","    total = 0\n","\n","    cmt = {}\n","    true_positive = 0\n","    true_negative = 0\n","    false_positive = 0\n","    false_negative = 0\n","    \n","    subj_predicted = {}\n","    subj_predicted['label'] = []\n","    subj_predicted['pred'] = []\n","    \n","    for i, data in enumerate(testloader,0):\n","        image, label = data\n","        image = image.to(f'cuda:{net.device_ids[0]}')\n","        label = label.to(f'cuda:{net.device_ids[0]}')\n","        output = net(image)\n","\n","        _, predicted = torch.max(output.data,1)\n","        total += label.size(0)\n","        correct += (predicted == label).sum().item()\n","        \n","        # calculate confusion_matrix\n","        result_cmt = confusion_matrix(label.cpu(), predicted.cpu())\n","\n","        if len(result_cmt) == 1:\n","            if label.item() ==1:\n","                true_positive += 1\n","            else:\n","                true_negative += 1\n","        else:\n","\n","            tn, fp, fn, tp = result_cmt.ravel()\n","            true_positive += int(tp)\n","            true_negative += int(tn)\n","            false_positive += int(fp)\n","            false_negative += int(fn)\n","        \n","        \n","        cmt['true_positive'] = true_positive\n","        cmt['true_negative'] = true_negative\n","        cmt['false_positive'] = false_positive\n","        cmt['false_negative'] = false_negative\n","\n","        # subj_predicted\n","        subj_predicted['label'].append(label.cpu().tolist()[0])\n","        subj_predicted['pred'].append(output.data.cpu().tolist()[0])\n","        #print(subj_predicted)\n","           \n","    test_acc = 100 * correct / total\n","    \n","    return test_acc, cmt, subj_predicted\n","## ============================================ ##"],"metadata":{"id":"g_IXz7jshOh8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## ========= Experiment =============== ##\n","def experiment(partition, args): #in_channels,out_dim\n","    \n","    if args.model == 'resnet50':\n","        net = resnet3d.resnet3D50()\n","    elif args.model == 'resnet101':\n","        net = resnet3d.resnet3D101()\n","    elif args.model == 'resnet152':\n","        net = resnet3d.resnet3D152()\n","\n","    net = torch.nn.DataParallel(net,device_ids=args.gpu_ids)\n","    net = net.to(f'cuda:{net.device_ids[0]}') #net = net.to(device)\n","        \n","    criterion = nn.CrossEntropyLoss()\n","    if args.optim == 'SGD':\n","        optimizer = optim.SGD(net.parameters(), lr=args.lr, momentum=args.momentum)\n","    elif args.optim == 'Adam':\n","        optimizer = optim.Adam(net.parameters(),lr=args.lr,weight_decay=args.weight_decay)\n","    else:\n","        raise ValueError('In-valid optimizer choice')\n","    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n","\n","    train_losses = []\n","    train_accs = []\n","    val_losses = []\n","    val_accs = []\n","\n","\n","    for epoch in tqdm(range(args.epoch)):\n","        ts = time.time()\n","        net, train_loss, train_acc = train(net,partition,optimizer,criterion,args)\n","        val_loss, val_acc = validate(net,partition,criterion, scheduler,args)\n","        test_acc = test(net,partition,args)\n","        te = time.time()\n","\n","        train_losses.append(train_loss)\n","        train_accs.append(train_acc)\n","        val_losses.append(val_loss)\n","        val_accs.append(val_acc)\n","\n","        print('Epoch {}, ACC(train/val): {:2.2f}/{:2.2f}, Loss(train/val): {:2.2f}/{:2.2f}. Current learning rate {}.Took {:2.2f} sec'.format(epoch,train_acc,val_acc,train_loss,val_loss,optimizer.param_groups[0]['lr'],te-ts))\n","\n","\n","    test_acc, cmt, subj_predicted = test(net,partition,args)\n","\n","    result = {}\n","    result['train_losses'] = train_losses\n","    result['train_accs'] = train_accs\n","    result['val_losses'] = val_losses\n","    result['val_accs'] = val_accs\n","    result['train_acc'] = train_acc\n","    result['val_acc'] = val_acc\n","    result['test_acc'] = test_acc\n","\n","    return vars(args), result, cmt, subj_predicted\n","## ==================================== ##"],"metadata":{"id":"A1vEahPohd5A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","## ========= Run Experiment and saving result ========= ##\n","# define result-saving function\n","def save_exp_result(setting, result, cmt, subj_predicted):\n","    exp_name = setting['exp_name']\n","    \n","    del setting['epoch']\n","    del setting['test_batch_size']\n","\n","    hash_key = hashlib.sha1(str(setting).encode()).hexdigest()[:6]\n","    filename = f'/home/connectome/lumierej/3DCNN/suicidality/{}--{}.json'.format(exp_name, hash_key)\n","    result.update(setting)\n","    result.update(cmt)\n","    result.update(subj_predicted)\n","    \n","    with open(filename, 'w') as f:\n","        json.dump(result, f)\n","\n","# seed number\n","seed = 1234\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","\n","# Run Experiment and save result\n","setting, result, cmt, subj_predicted = experiment(partition, deepcopy(args))\n","save_exp_result(setting,result, cmt, subj_predicted)\n","## ==================================================== ##"],"metadata":{"id":"vUBU6pBLhgd8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["python3 run_resnet3d_gpu.py --model densenet121 --optim Adam --lr 1e-6 --epoch 100 --exp_name test --gpu_ids 6 7 --train_batch_size 16 --val_batch_size 16 --test_batch_size 1"],"metadata":{"id":"is_u6hB5hjas"},"execution_count":null,"outputs":[]}]}