{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"resnet.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPsPPqoYIE1QXoFUlCiLVxM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"It1ilhkjNMeN"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torch.backends.cudnn as cudnn\n","import numpy as np\n","\n","from tqdm.auto import tqdm\n","import torchvision\n","import torchvision.transforms as transforms\n","import datetime\n","import os\n","from torch.utils.tensorboard import SummaryWriter\n","import shutil"]},{"cell_type":"code","source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","# Data\n","print('==> Preparing data..')\n","transform_train = transforms.Compose([\n","    transforms.RandomCrop(32, padding=4),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","])\n","\n","transform_test = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","])\n","\n","trainset = torchvision.datasets.CIFAR10(\n","    root='/content/drive/MyDrive/CIFAR10', train=True, download=True, transform=transform_train)\n","trainloader = torch.utils.data.DataLoader(\n","    trainset, batch_size=128, shuffle=True, num_workers=2)\n","\n","testset = torchvision.datasets.CIFAR10(\n","    root='/content/drive/MyDrive/CIFAR10', train=False, download=True, transform=transform_test)\n","testloader = torch.utils.data.DataLoader(\n","    testset, batch_size=100, shuffle=False, num_workers=2)\n","\n","current_time = datetime.datetime.now() + datetime.timedelta(hours= 9)\n","current_time = current_time.strftime('%Y-%m-%d-%H:%M')\n","\n","saved_loc = os.path.join('/content/drive/MyDrive/ResNet_Result', current_time)\n","if os.path.exists(saved_loc):\n","    shutil.rmtree(saved_loc)\n","os.mkdir(saved_loc)\n","\n","print(\"결과 저장 위치: \", saved_loc)\n","\n","writer = SummaryWriter(saved_loc)"],"metadata":{"id":"lPAqapv1NQXF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Model\n","print('==> Building model..')\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = nn.Conv2d(\n","            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n","                               stride=1, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes,\n","                          kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n","                               stride=stride, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion *\n","                               planes, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes,\n","                          kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = F.relu(self.bn2(self.conv2(out)))\n","        out = self.bn3(self.conv3(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class ResNet(nn.Module):\n","    def __init__(self, block, num_blocks, num_classes=10):\n","        super(ResNet, self).__init__()\n","        self.in_planes = 64\n","\n","        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n","                               stride=1, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","        self.linear = nn.Linear(512*block.expansion, num_classes)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.layer1(out)\n","        out = self.layer2(out)\n","        out = self.layer3(out)\n","        out = self.layer4(out)\n","        out = F.avg_pool2d(out, 4)\n","        out = out.view(out.size(0), -1)\n","        out = self.linear(out)\n","        return out\n","\n","#net = ResNet(BasicBlock, [2, 2, 2, 2]) # ResNet18\n","#net = ResNet(BasicBlock, [3, 4, 6, 3]) # ResNet34\n","net = ResNet(Bottleneck, [3, 4, 6, 3]) # ResNet50\n","\n","net = net.to(device)\n","cudnn.benchmark = True\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(net.parameters(), lr=0.1,\n","                      momentum=0.9, weight_decay=5e-4)\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n","\n","best_acc = 0"],"metadata":{"id":"OP31h0RnNTH3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Training\n","def train(epoch):\n","    print('\\nEpoch: %d' % epoch)\n","    net.train()\n","    train_loss = 0\n","    correct = 0\n","    total = 0\n","    batch_count = 0\n","    for batch_idx, (inputs, targets) in enumerate(trainloader):\n","        inputs, targets = inputs.to(device), targets.to(device)\n","        optimizer.zero_grad()\n","        outputs = net(inputs)\n","        loss = criterion(outputs, targets)\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","        _, predicted = outputs.max(1)\n","        total += targets.size(0)\n","        correct += predicted.eq(targets).sum().item()\n","        batch_count += 1\n","\n","    print(\"Train Loss : {:.3f} | Train Acc: {:.3f}\".format(train_loss / batch_count, 100.*correct/total))\n","    final_loss = train_loss / batch_count\n","    final_acc = 100.*correct / total\n","    return final_loss, final_acc\n","\n","\n","def test(epoch):\n","    global best_acc\n","    net.eval()\n","    test_loss = 0\n","    correct = 0\n","    total = 0\n","    batch_count = 0\n","    with torch.no_grad():\n","        for batch_idx, (inputs, targets) in enumerate(testloader):\n","            inputs, targets = inputs.to(device), targets.to(device)\n","            outputs = net(inputs)\n","            loss = criterion(outputs, targets)\n","\n","            test_loss += loss.item()\n","            _, predicted = outputs.max(1)\n","            total += targets.size(0)\n","            correct += predicted.eq(targets).sum().item()\n","            batch_count += 1\n","\n","\n","    print(\"Test Loss : {:.3f} | Test Acc: {:.3f}\".format(test_loss / batch_count, 100.*correct/total))\n","\n","    # Save checkpoint.\n","    acc = 100.*correct/total\n","    if acc > best_acc:\n","        print('Saving..')\n","        state = {\n","            'net': net.state_dict(),\n","            'acc': acc,\n","            'epoch': epoch,\n","        }\n","        if not os.path.isdir(os.path.join(saved_loc, 'checkpoint')):\n","            os.mkdir(os.path.join(saved_loc, 'checkpoint'))\n","        torch.save(state, os.path.join(saved_loc, 'checkpoint/ckpt.pth'))\n","        best_acc = acc\n","    \n","    final_loss = test_loss / batch_count\n","    return final_loss, acc"],"metadata":{"id":"cJv3nZvbNU9m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for epoch in tqdm(range(200)):\n","    train_loss, train_acc = train(epoch)\n","    test_loss, test_acc = test(epoch)\n","\n","    writer.add_scalars('Loss', {\"Train Loss\" : train_loss, \"Test Loss\" : test_loss}, epoch)\n","    writer.add_scalars('Accuracy', {\"Train acc\" : train_acc, \"test acc\" : test_acc}, epoch)\n","    \n","    scheduler.step()\n","\n","writer.close()"],"metadata":{"id":"8HLOPZ16NVjz"},"execution_count":null,"outputs":[]}]}